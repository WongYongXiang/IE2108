{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Qn 2 using of Kruskal's algorithm to find the Minimum Spanning Tree(MST) with added feature of calculating the cost\n",
    "\n",
    "class DisjointSet:\n",
    "    def __init__(self, vertices):\n",
    "        self.parent = [-1] * vertices    #initialising all vertices as disjoint at the start\n",
    "    \n",
    "    def find(self, node):    #finding the root of a set to which the vertex belongs\n",
    "        if self.parent[node] == -1:\n",
    "            return node\n",
    "        return self.find(self.parent[node])\n",
    "    \n",
    "    def union(self, x, y):   #Combining 2 sets together where the root of one set \n",
    "        x_set = self.find(x) #is the pointting to the parent of the other set\n",
    "        y_set = self.find(y)\n",
    "        if x_set != y_set:\n",
    "            self.parent[x_set] = y_set\n",
    "\n",
    "class Graph:\n",
    "    def __init__(self, vertices):\n",
    "        self.V = vertices\n",
    "        self.adjacency_list = []\n",
    "\n",
    "    def add_edge(self, u, v, weight):\n",
    "        self.adjacency_list.append((u, v, weight))  \n",
    "\n",
    "    def kruskal(self):\n",
    "        self.adjacency_list.sort(key = lambda x:x[2]) #sort the edges in the list by weight, which is defined as key in the parameter\n",
    "        disjoint_set = DisjointSet(self.V) #Think of it as all individual verices being on its own set initially as it shouldnt have parents nor child\n",
    "        mst = []\n",
    "        cost = 0 \n",
    "\n",
    "        for edge in self.adjacency_list:\n",
    "            u, v, weight = edge\n",
    "            u_set = disjoint_set.find(u)\n",
    "            v_set = disjoint_set.find(v)\n",
    "\n",
    "            if u_set != v_set:  #if the the tree of u is not connected to v already, connect them\n",
    "                mst.append((u, v, weight))         #add edge to MST\n",
    "                disjoint_set.union(u, v)\n",
    "                cost += weight #update the cost of the tree\n",
    "        \n",
    "        return mst\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Graph(16) #16 vertices\n",
    " graph.add_edge(1, 2, 5)\n",
    " graph.add_edge(0, 2, 2)\n",
    " graph.add_edge(0, 4, 3)\n",
    " graph.add_edge(1, 3, 5)\n",
    " graph.add_edge(2, 3, 1)\n",
    " graph.add_edge(2, 4, 6)\n",
    " graph.add_edge(2, 5, 3)\n",
    " graph.add_edge(3, 5, 6)\n",
    " graph.add_edge(4, 5, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qn 3 \n",
    "\n",
    "Mini-Project: Sentiment Analysis on Movie Reviews \n",
    "Objective: \n",
    "Build a text classification model to classify movie reviews as positive or negative based on \n",
    "their sentiment. \n",
    " \n",
    "Data: \n",
    "We'll use a small dataset of movie reviews from the NLTK library, which is a Python \n",
    "library for natural language processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\movie_reviews.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "nltk.download(\"movie_reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random         # used for shuffling the data\n",
    "from nltk import word_tokenize      \n",
    "# used to tokenize text into words \n",
    "from nltk.corpus import movie_reviews, stopwords    # common stopwords \n",
    "from nltk.classify import NaiveBayesClassifier \n",
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load movie reviews\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# Shuffle the documents\n",
    "random.shuffle(documents)\n",
    "\n",
    "# Define stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define a function to extract features from the text\n",
    "def document_features(document):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the most frequent words as features\n",
    "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words() if w.lower() not in stop_words)\n",
    "word_features = list(all_words.keys())[:2000]\n",
    "\n",
    "# Extract features from the documents\n",
    "featuresets = [(document_features(d), c) for (d, c) in documents]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_set, test_set = featuresets[:1500], featuresets[1500:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy: 0.812\n"
     ]
    }
   ],
   "source": [
    "# Train the Naive Bayes classifier\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print(\"Classifier accuracy:\", nltk.classify.accuracy(classifier, test_set))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
